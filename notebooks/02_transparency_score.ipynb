{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv5V8o0ZRSH4mIswIINA+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shravani018/llm-audit-bench/blob/main/notebooks/02_transparency_score.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 02. Transparency Score\n",
        "\n",
        "**Loading model's card text and scoring it aginst 7 completeness criteria.**"
      ],
      "metadata": {
        "id": "6VpE9LYSvRcJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GjdoicUIu1Rq"
      },
      "outputs": [],
      "source": [
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from huggingface_hub import HfApi,ModelCard"
      ],
      "metadata": {
        "id": "20Yr2j5dwDvY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLMs used\n",
        "models=[\n",
        "    \"gpt2\",\n",
        "    \"distilgpt2\",\n",
        "    \"facebook/opt-125m\",\n",
        "    \"EleutherAI/gpt-neo-125m\",\n",
        "    \"bigscience/bloom-560m\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "TcCWo9QJwWje"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the criteris and weights(summing to 1.0)\n",
        "criteria={\n",
        "    \"has_model_card\":0.20,\n",
        "    \"license\":0.15,\n",
        "    \"training_data\":0.20,\n",
        "    \"limitations\":0.15,\n",
        "    \"intended_use\":0.10,\n",
        "    \"evaluation_results\":0.10,\n",
        "    \"carbon_footprint\":0.10\n",
        "}"
      ],
      "metadata": {
        "id": "BGP005-Iwbi2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing API\n",
        "api=HfApi()"
      ],
      "metadata": {
        "id": "6rLPKRXjw5sz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching model card\n",
        "def get_model_card(model_id):\n",
        "    try:\n",
        "        model_card=ModelCard.load(model_id)\n",
        "        return model_card.content.lower()\n",
        "    except Exception:\n",
        "      return None"
      ],
      "metadata": {
        "id": "nMSvKcQTxBh4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for license\n",
        "def check_license(model_id):\n",
        "    try:\n",
        "        license=api.model_info(model_id)\n",
        "        if license.cardData and license.cardData.get(\"license\"):\n",
        "          return True\n",
        "    except Exception:\n",
        "      return False"
      ],
      "metadata": {
        "id": "gqipiYoSxSA_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining scoring mechanism for the model\n",
        "def score_model_card(card_text,model_id):\n",
        "  if card_text is None:\n",
        "    return {k:False for k in criteria}\n",
        "  checks={}\n",
        "  # Does the card exist?\n",
        "  checks[\"has_model_card\"]=True #already extracted the text so it'll be True\n",
        "  # Does the model have a license?\n",
        "  checks['license']=check_license(model_id)\n",
        "  # Is the training data described?\n",
        "  checks[\"training_data\"] = any(kw in card_text for kw in\n",
        "        [\"trained on\", \"training data\", \"dataset\", \"corpus\", \"pretraining\", \"fine-tuned on\"])\n",
        "  # Are the limitations mentioned?\n",
        "  checks[\"limitations\"] = any(kw in card_text for kw in\n",
        "        [\"limitation\", \"bias\", \"risk\", \"not suitable\", \"avoid\", \"failure\", \"caveat\"])\n",
        "  # Is the intended use described?\n",
        "  checks[\"intended_use\"] = any(kw in card_text for kw in\n",
        "        [\"intended use\", \"use case\", \"designed for\", \"primary use\", \"out-of-scope\", \"downstream\"])\n",
        "  # Are the evaluation results present?\n",
        "  checks[\"evaluation_results\"] = any(kw in card_text for kw in\n",
        "        [\"benchmark\", \"accuracy\", \"f1\", \"perplexity\", \"bleu\", \"rouge\", \"results\", \"performance\", \"score\"])\n",
        "  # Are the costs or carbon footprint mentioned?\n",
        "  checks[\"carbon_footprint\"] = any(kw in card_text for kw in\n",
        "        [\"carbon\", \"co2\", \"emissions\", \"compute\", \"gpu hours\", \"energy\", \"environmental\"])\n",
        "  return checks\n"
      ],
      "metadata": {
        "id": "hFkR7a8AxyiH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the score\n",
        "def calc_score(checks):\n",
        "  score=round(sum(criteria[k]*(1.0 if checks[k] else 0.0) for k in criteria),4)\n",
        "  return score"
      ],
      "metadata": {
        "id": "hpVqtc8jzRng"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scoring the model\n",
        "def eval_transperancy(model_id):\n",
        "  print(f\"Evaluating Transperancy for:{model_id}\")\n",
        "  card_text=get_model_card(model_id)\n",
        "  checks=score_model_card(card_text,model_id)\n",
        "  score=calc_score(checks)\n",
        "  stats={\"model_id\":model_id,\"checks\":checks,\"score\":score}\n",
        "  return stats"
      ],
      "metadata": {
        "id": "wsTa9-iuzikg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating transperancy for each model\n",
        "model_transperancy=[eval_transperancy(model_id) for model_id in models]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAOeurxgz9gM",
        "outputId": "dcec875a-55ad-4a03-b525-9178567a1d78"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Transperancy for:gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Transperancy for:distilgpt2\n",
            "Evaluating Transperancy for:facebook/opt-125m\n",
            "Evaluating Transperancy for:EleutherAI/gpt-neo-125m\n",
            "Evaluating Transperancy for:bigscience/bloom-560m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for r in model_transperancy:\n",
        "    row = {\"model_id\": r[\"model_id\"], \"transparency_score\": r[\"score\"]}\n",
        "    row.update(r[\"checks\"])  # flatten checks into columns\n",
        "    rows.append(row)"
      ],
      "metadata": {
        "id": "mrmjJO900Jhb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(rows)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "RLhAmHCE0QIV",
        "outputId": "9429e41c-578a-4687-d521-490fbc21fb02"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  model_id  transparency_score  has_model_card  license  \\\n",
              "0                     gpt2                 0.9            True     True   \n",
              "1               distilgpt2                 1.0            True     True   \n",
              "2        facebook/opt-125m                 0.9            True     True   \n",
              "3  EleutherAI/gpt-neo-125m                 0.9            True     True   \n",
              "4    bigscience/bloom-560m                 1.0            True     True   \n",
              "\n",
              "   training_data  limitations  intended_use  evaluation_results  \\\n",
              "0           True         True          True                True   \n",
              "1           True         True          True                True   \n",
              "2           True         True          True                True   \n",
              "3           True         True          True                True   \n",
              "4           True         True          True                True   \n",
              "\n",
              "   carbon_footprint  \n",
              "0             False  \n",
              "1              True  \n",
              "2             False  \n",
              "3             False  \n",
              "4              True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7eb76719-ab44-4c71-a4ba-9a1aba022d43\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_id</th>\n",
              "      <th>transparency_score</th>\n",
              "      <th>has_model_card</th>\n",
              "      <th>license</th>\n",
              "      <th>training_data</th>\n",
              "      <th>limitations</th>\n",
              "      <th>intended_use</th>\n",
              "      <th>evaluation_results</th>\n",
              "      <th>carbon_footprint</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gpt2</td>\n",
              "      <td>0.9</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>distilgpt2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>facebook/opt-125m</td>\n",
              "      <td>0.9</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EleutherAI/gpt-neo-125m</td>\n",
              "      <td>0.9</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bigscience/bloom-560m</td>\n",
              "      <td>1.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7eb76719-ab44-4c71-a4ba-9a1aba022d43')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7eb76719-ab44-4c71-a4ba-9a1aba022d43 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7eb76719-ab44-4c71-a4ba-9a1aba022d43');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"model_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"distilgpt2\",\n          \"bigscience/bloom-560m\",\n          \"facebook/opt-125m\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transparency_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0547722557505166,\n        \"min\": 0.9,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"has_model_card\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"license\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"training_data\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"limitations\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"intended_use\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evaluation_results\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"carbon_footprint\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./transparency_scores.json\", \"w\") as f:\n",
        "    json.dump({\"transparency\": model_transperancy}, f, indent=2)"
      ],
      "metadata": {
        "id": "jPggQ1Bj0rU7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "- All 5 models score between 0.9 and 1.0 on transparency, indicating strong documentation practices across the board.\n",
        "- This is expected given that these are widely adopted, community-maintained models from established organisations where documentation standards are high.\n",
        "- The only failing criterion across all models is carbon footprint disclosure, with only `distilgpt2` and `bloom-560m` reporting environmental cost.\n",
        "- Carbon reporting in ML only became a community norm post-2022, explaining why older models like `gpt2` and `gpt-neo-125m` omit it entirely.\n",
        "- Transparency is the easiest pillar to score well on, it reflects documentation quality, not model behaviour, making it a weak proxy for true trustworthiness.\n",
        "- The high scores here set a strong baseline but should not be interpreted as an indicator of fairness, robustness, or safety."
      ],
      "metadata": {
        "id": "fxz8U1LG2hIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next 03_fairness_Score.ipynb\n",
        "\n",
        "Measuring stereotype bias across 5 demographic categories using CrowS-Pairs log-probability comparison."
      ],
      "metadata": {
        "id": "uUVWpvUj3nzy"
      }
    }
  ]
}