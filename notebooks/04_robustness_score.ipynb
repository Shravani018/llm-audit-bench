{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Shravani018/llm-audit-bench/blob/main/notebooks/04_robustness_score.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 04: Robustness Score\n",
        "Measuring each model's stability under input perturbations by computing perplexity shift across typo, word deletion, and synonym substitution attacks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CSpEsha125Hb"
      },
      "outputs": [],
      "source": [
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C0HJnBm_3Oi5"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import nltk\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "from nltk.corpus import wordnet\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OYhDWbQD3SPV"
      },
      "outputs": [],
      "source": [
        "# LLMs used\n",
        "models=[\n",
        "    \"gpt2\",\n",
        "    \"distilgpt2\",\n",
        "    \"facebook/opt-125m\",\n",
        "    \"EleutherAI/gpt-neo-125m\",\n",
        "    \"bigscience/bloom-560m\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bneGcxbM3c6g",
        "outputId": "ea778824-b9f7-427a-9408-c22cf459364d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        }
      ],
      "source": [
        "#Loading 100 sentences from SST-2\n",
        "sst2=load_dataset(\"sst2\", split=\"validation\")\n",
        "sentences=[row[\"sentence\"] for row in sst2.select(range(100))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OtKBR7sa3pBQ"
      },
      "outputs": [],
      "source": [
        "# finds synonyms\n",
        "def get_synonym(word):\n",
        "    synsets=wordnet.synsets(word)\n",
        "    for syn in synsets:\n",
        "        for lemma in syn.lemmas():\n",
        "            candidate=lemma.name().replace(\"_\", \" \")\n",
        "            if candidate.lower()!=word.lower():\n",
        "                return candidate\n",
        "    return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dF4tSq9C4r8v"
      },
      "outputs": [],
      "source": [
        "#Replaces the first word in the sentence with that synonym\n",
        "def perturb_synonym(sentence):\n",
        "    words=sentence.split()\n",
        "    result=words[:]\n",
        "    for i, word in enumerate(words):\n",
        "        clean=word.lower().strip(string.punctuation)\n",
        "        syn=get_synonym(clean)\n",
        "        if syn!=clean:\n",
        "            result[i]=syn\n",
        "            break\n",
        "    return \" \".join(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yUq6mw-_4gMy"
      },
      "outputs": [],
      "source": [
        "# selects a random word and switches two chars to create a typo\n",
        "def perturb_typo(sentence):\n",
        "    words=sentence.split()\n",
        "    if not words:\n",
        "        return sentence\n",
        "    idx=random.randint(0, len(words) - 1)\n",
        "    word=list(words[idx])\n",
        "    if len(word) > 2:\n",
        "        swap=random.randint(0, len(word) - 2)\n",
        "        word[swap], word[swap + 1] = word[swap + 1], word[swap]\n",
        "    words[idx]=\"\".join(word)\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Qf3T0JC34ltS"
      },
      "outputs": [],
      "source": [
        "# removes a random word from the sentence\n",
        "def perturb_delete(sentence):\n",
        "    words=sentence.split()\n",
        "    if len(words) <= 1:\n",
        "        return sentence\n",
        "    idx=random.randint(0, len(words) - 1)\n",
        "    words.pop(idx)\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BRPbPmAC4wuh"
      },
      "outputs": [],
      "source": [
        "perturbations={\n",
        "    \"typo\":    perturb_typo,\n",
        "    \"deletion\": perturb_delete,\n",
        "    \"synonym\": perturb_synonym\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MlbR7DIm42G7"
      },
      "outputs": [],
      "source": [
        "#runs the sentence through the model and returns its perplexity\n",
        "def get_perplexity(model, tokenizer, sentence, device):\n",
        "    inputs = tokenizer(\n",
        "        sentence, return_tensors=\"pt\",\n",
        "        truncation=True, max_length=128\n",
        "    ).to(device)\n",
        "    if inputs[\"input_ids\"].shape[1] < 2:\n",
        "        return None\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    return math.exp(outputs.loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hvKm69eA4-ZM"
      },
      "outputs": [],
      "source": [
        "#Applying all 4 perturbations to each sentence, measures the perplexity shift before and after, and returns an overall robustness score plus per-perturbation breakdown.\n",
        "def compute_robustness(model, tokenizer, sentences, device):\n",
        "    per_type_shifts={k: [] for k in perturbations}\n",
        "    all_shifts=[]\n",
        "    for sentence in tqdm(sentences, desc=\"scoring sentences\"):\n",
        "        ppl_orig=get_perplexity(model, tokenizer, sentence, device)\n",
        "        if ppl_orig is None or ppl_orig==0:\n",
        "            continue\n",
        "        for ptype, pfunc in perturbations.items():\n",
        "            perturbed=pfunc(sentence)\n",
        "            ppl_pert=get_perplexity(model, tokenizer, perturbed, device)\n",
        "            if ppl_pert is None:\n",
        "                continue\n",
        "            shift=abs(ppl_pert - ppl_orig) / ppl_orig\n",
        "            per_type_shifts[ptype].append(shift)\n",
        "            if ptype != \"shuffle\":\n",
        "                all_shifts.append(shift)\n",
        "    mean_shift=float(np.mean(all_shifts)) if all_shifts else 1.0\n",
        "    robustness_score=round(max(0.0, 1.0 - min(mean_shift, 1.0)), 4)\n",
        "    per_type_scores={\n",
        "        ptype: round(max(0.0, 1.0 - min(float(np.mean(shifts)), 1.0)), 4)\n",
        "        if shifts else None\n",
        "        for ptype, shifts in per_type_shifts.items()\n",
        "        if ptype != \"shuffle\"\n",
        "    }\n",
        "    return robustness_score, mean_shift, per_type_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8RMz5SM05OLN"
      },
      "outputs": [],
      "source": [
        "# Evaluating robustness for each model\n",
        "def evaluate_robustness(model_id, sentences):\n",
        "    print(f\"\\nEvaluating: {model_id}\")\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
        "    model=AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float32)\n",
        "    model=model.to(device)\n",
        "    model.eval()\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    robustness_score, mean_shift, per_type = compute_robustness(\n",
        "        model, tokenizer, sentences, device)\n",
        "    del model\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    print(f\"Robustness:{robustness_score}|mean shift: {round(mean_shift, 4)}\")\n",
        "    print(f\"per type:{per_type}\")\n",
        "    return {\n",
        "        \"model_id\":        model_id,\n",
        "        \"robustness_score\": robustness_score,\n",
        "        \"mean_shift\":       round(mean_shift, 4),\n",
        "        \"sentences_tested\": len(sentences),\n",
        "        \"per_perturbation\": per_type,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6nAkpesH5Rg9",
        "outputId": "ac67e0d7-7ee2-435b-ef65-38f3ddd7a450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: gpt2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b624271b99474e5fb0ffe75460ac3760",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
            "Key                  | Status     |  | \n",
            "---------------------+------------+--+-\n",
            "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "scoring sentences:   0%|          | 0/100 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
            "scoring sentences: 100%|██████████| 100/100 [02:16<00:00,  1.36s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robustness:0.4577|mean shift: 0.5423\n",
            "per type:{'typo': 0.3625, 'deletion': 0.4892, 'synonym': 0.5213}\n",
            "\n",
            "Evaluating: distilgpt2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de85097279144c89b4cdf0940ead78f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: distilgpt2\n",
            "Key                                        | Status     |  | \n",
            "-------------------------------------------+------------+--+-\n",
            "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "scoring sentences: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robustness:0.4535|mean shift: 0.5465\n",
            "per type:{'typo': 0.2557, 'deletion': 0.4658, 'synonym': 0.6389}\n",
            "\n",
            "Evaluating: facebook/opt-125m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff70e1a94ba54ad9908cfa05a9eb97c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tied weights mapping and config for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "scoring sentences: 100%|██████████| 100/100 [01:20<00:00,  1.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robustness:0.201|mean shift: 0.799\n",
            "per type:{'typo': 0.0186, 'deletion': 0.4343, 'synonym': 0.1503}\n",
            "\n",
            "Evaluating: EleutherAI/gpt-neo-125m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf19bdbe3b5d4607a1d2ce6d3066ea00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e2e6bfeecf7480f88423e243821f816",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f84db593fe5147b78bb524b0cfd45875",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e27bedc22c034710bf59c367e2452f62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "038cb3a964c24f048f5875c1c86e4c0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6576a195c435499ea2431a445be588f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bb8d15c49f84944a5e536549cf1e991",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfb8f50a68064880b6f9d15dc6e75e7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/160 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPTNeoForCausalLM LOAD REPORT from: EleutherAI/gpt-neo-125m\n",
            "Key                                                   | Status     |  | \n",
            "------------------------------------------------------+------------+--+-\n",
            "transformer.h.{0...11}.attn.attention.masked_bias     | UNEXPECTED |  | \n",
            "transformer.h.{0, 2, 4, 6, 8, 10}.attn.attention.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37f74b557a0b41028e290ed787456d58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "scoring sentences: 100%|██████████| 100/100 [01:25<00:00,  1.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robustness:0.3278|mean shift: 0.6722\n",
            "per type:{'typo': 0.2035, 'deletion': 0.3312, 'synonym': 0.4486}\n",
            "\n",
            "Evaluating: bigscience/bloom-560m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c2d5bd6e46f409aa5a863e232838538",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90ede0f4fe394dfea1d7bddc8041a56e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af634edce36f4490ac4486a94a375172",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ced0af744ae402e969ef21b62640e67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb82ef00f71243f5a23e5c23f5c4a515",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c53c40020444cb89af769af954cc03b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/293 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "scoring sentences: 100%|██████████| 100/100 [06:28<00:00,  3.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robustness:0.0|mean shift: 4.6668\n",
            "per type:{'typo': 0.0, 'deletion': 0.638, 'synonym': 0.7409}\n",
            "\n",
            "Evaluated 5 models.\n"
          ]
        }
      ],
      "source": [
        "results = [evaluate_robustness(m, sentences) for m in models]\n",
        "print(f\"\\nEvaluated {len(results)} models.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9Wr54OTG9GB6"
      },
      "outputs": [],
      "source": [
        "with open(\"./robustness_scores.json\", \"w\") as f:\n",
        "    json.dump({\"robustness\": results}, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7a_aJXvzYOp"
      },
      "source": [
        "- `gpt2` and `distilgpt2` are the most robust overall at 0.46 and 0.45, stable under most perturbation types.\n",
        "- `distilgpt2` despite being a compressed version of `gpt2` performs comparably, suggesting compression did not hurt robustness.\n",
        "- `gpt-neo-125m` sits in the middle at 0.33 with consistent but mediocre scores across all three perturbation types.\n",
        "- `opt-125m` scores 0.20 overall, largely due to a typo score of 0.02, indicating extreme sensitivity to character-level changes.\n",
        "- `bloom-560m` scores 0.0 due to a mean shift of 4.67, driven by typo sensitivity causing perplexity to spike well beyond the 1.0 normalisation cap.\n",
        "- **Synonym substitution** shows the most variation across models, ranging from 0.15 to 0.74, making it the most informative perturbation type.\n",
        "- Deletion scores are consistent across all models, suggesting word removal is handled similarly regardless of architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFvhLpl-zszQ"
      },
      "source": [
        "Next: 05_explainability_score.ipynb\n",
        "Measures how interpretable each model's predictions are by computing SHAP token attribution scores over a set of input sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXUbvJPiz7uY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
